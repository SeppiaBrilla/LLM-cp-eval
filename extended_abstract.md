## Old
Since the release of ChatGPT, large language models (LLMs) have been adopted across a wide range of research domains, such as biomedical sciences, physics, and the social sciences. In the field of Constraint Programming (CP), LLMs have been applied to the modelling process. In CP, a model is a formal specification of a problem, expressed in terms of parameters, variables, and constraints. Once a CP model has been formalised, usually in a  
To assess the modelling capabilities of LLMs on CP problems, the community has developed benchmark datasets such as CPEVAL (https://github.com/Yuliang795/LLMs-CP-CPEVAL/tree/main) and CP-Bench (https://huggingface.co/datasets/kostis-init/CP-Bench). Since LLMs often achieve remarkable results on these benchmarks, this raises the issue of data contamination: many CP problems are well-known and have been widely studied, meaning that correct models may already be present in the training data. As a result, benchmark performance might not reflect genuine reasoning capabilities.
In this study, we investigate whether data contamination exists and how it affects results. To this end, we rephrased a selection of CP problems from CSPLib (https://www.csplib.org/) to preserve their underlying structure while altering their textual descriptions. Additionally, we introduced non-essential “distractions” into the problem statements. These distractions do not alter the problem itself but make its objective less immediately clear.
We then evaluated three LLMs: GPT-4, DeepSeek-R1, and Claude-4 in a zero-shot setting (i.e., without allowing the models to iteratively refine or debug their results). For each problem, we compared models generated from the original descriptions (om) against those generated from the modified descriptions (mm). The resulting MiniZinc models (https://www.minizinc.org/) were manually assessed across three dimensions:

- Runs: whether the model executes successfully (with no or only minor modifications).
- Globals: the use of global constraints, which typically signal higher-quality modelling.
- Correctness: whether the model addresses the correct problem.

Our findings show that for all LLMs, both correctness and globals scores were consistently higher for om than for mm. This suggests that the models may heavily rely on training data. Indeed, the om outputs were generally more elaborate, featuring sophisticated data representations and optimisation “tricks” commonly used by expert modellers.
Conversely, the runs metric tended to be higher for mm, likely due to the simpler modelling choices made in these cases. Interestingly, only in one instance did a LLM (DeepSeek-R1) recognise the similarity between a modified description and its well-known original counterpart; in all other cases, LLMs failed to make this connection. This points to a lack of generalisation, which may limit their effectiveness in real-world applications where problems are presented in unfamiliar forms.
All data and resources from this study are available on GitHub: https://github.com/SeppiaBrilla/LLM-cp-eval/tree/master.

---

## New
Since the release of ChatGPT, large language models (LLMs) have been adopted across a wide range of research domains, such as biomedical sciences, physics, and the social sciences. In the field of Constraint Programming (CP), LLMs have been applied to the modelling process. In CP, a model is a formal specification of a problem, expressed in terms of parameters, variables, and constraints. Once a CP model has been formalised, usually in a modelling language such as Minizinc (https://www.minizinc.org/) or CPMpy (https://cpmpy.readthedocs.io/en/latest/), it can then be fed to a solver that searches for a solution. We can categorise a problem into one of two categories: (i) satisfaction problems for which every solution is equivalent and (ii) optimisation problems for which we want to maximise or minimise an objective function. The modelling decisions taken, such as the number of variables or the constraints over them, can vastly impact the solving performance. Thus, it is critical to write an efficient model to represent a problem.
To assess the modelling capabilities of LLMs on CP problems, the community has developed benchmark datasets such as CPEVAL (https://github.com/Yuliang795/LLMs-CP-CPEVAL/tree/main) and CP-Bench (https://huggingface.co/datasets/kostis-init/CP-Bench). These benchmark are usually composed of a set of problems with a formal description written in natural language and a set of metrics to evaluate the answers. Since LLMs often achieve remarkable results on these benchmarks, this raises the issue of data contamination: many CP problems, such as TSP or Sudoku, are well-known and have been widely studied, meaning that correct models may already be present in the training data. As a result, benchmark performance might not reflect genuine reasoning capabilities.
In this study, we investigate whether data contamination exists and how it affects results. To this end, we rephrased a selection of CP problems from CSPLib (https://www.csplib.org/) to preserve their underlying structure while altering their textual descriptions. Additionally, we introduced non-essential “distractions” into the problem statements. These distractions do not alter the problem itself but make its objective less immediately clear.
We then evaluated three LLMs: GPT-4, DeepSeek-R1, and Claude-4 in a zero-shot setting (i.e., without allowing the models to iteratively refine or debug their results). For each problem, we compared models generated from the original descriptions (om) against those generated from the modified descriptions (mm). The resulting MiniZinc models (https://www.minizinc.org/) were manually assessed across three dimensions:

- Runs: whether the model executes successfully (with no or only minor modifications).
- Globals: the use of global constraints, which typically signal higher-quality modelling.
- Correctness: whether the model addresses the correct problem.

Our findings show that for all LLMs, both correctness and globals scores were consistently higher for om than for mm. 
Conversely, the runs metric tended to be higher for mm, likely due to the simpler modelling choices made in these models. Interestingly, only in one instance did a LLM recognise the similarity between a modified description and its original counterpart; in all other cases, LLMs failed to make this connection.  
All data and resources from this study are available on GitHub: https://github.com/SeppiaBrilla/LLM-cp-eval/tree/master.