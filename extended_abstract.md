Since the release of ChatGPT, large language models (LLMs) have been adopted across a wide range of research domains, such as biomedical sciences, physics, and the social sciences. In the field of Constraint Programming (CP), LLMs have been applied to the modelling process. In CP, a model is a formal specification of a problem, expressed in terms of parameters, variables, and constraints.
To assess the modelling capabilities of LLMs on CP problems, the community has developed benchmark datasets such as CPEVAL (https://github.com/Yuliang795/LLMs-CP-CPEVAL/tree/main) and CP-Bench (https://huggingface.co/datasets/kostis-init/CP-Bench). Since LLMs often achieve remarkable results on these benchmarks, this raises the issue of data contamination: many CP problems are well-known and have been widely studied, meaning that correct models may already be present in the training data. As a result, benchmark performance might not reflect genuine reasoning capabilities.
In this study, we investigate whether data contamination exists and how it affects results. To this end, we rephrased a selection of CP problems from CSPLib (https://www.csplib.org/) to preserve their underlying structure while altering their textual descriptions. Additionally, we introduced non-essential “distractions” into the problem statements. These distractions do not alter the problem itself but make its objective less immediately clear.
We then evaluated three LLMs: GPT-4, DeepSeek-R1, and Claude-4 in a zero-shot setting (i.e., without allowing the models to iteratively refine or debug their results). For each problem, we compared models generated from the original descriptions (om) against those generated from the modified descriptions (mm). The resulting MiniZinc models (https://www.minizinc.org/) were manually assessed across three dimensions:
- Runs: whether the model executes successfully (with no or only minor modifications).
- Globals: the use of global constraints, which typically signal higher-quality modelling.
- Correctness: whether the model addresses the correct problem.
Our findings show that for all LLMs, both correctness and globals scores were consistently higher for om than for mm. This suggests that the models may heavily rely on training data. Indeed, the om outputs were generally more elaborate, featuring sophisticated data representations and optimisation “tricks” commonly used by expert modellers.
Conversely, the runs metric tended to be higher for mm, likely due to the simpler modelling choices made in these cases. Interestingly, only in one instance did a LLM (DeepSeek-R1) recognise the similarity between a modified description and its well-known original counterpart; in all other cases, LLMs failed to make this connection. This points to a lack of generalisation, which may limit their effectiveness in real-world applications where problems are presented in unfamiliar forms.
All data and resources from this study are available on GitHub: https://github.com/SeppiaBrilla/LLM-cp-eval/tree/master.